# Metric Learning Score Prediction 
**Mohmad Yaqoob**  
**DA25M017**

M.Tech, IIT Madras  
DA5401 â€“ End Semester Challenge, 2025

##  Project Overview
 
This project focuses on **metric learning**, a sub-area of machine learning centered on learning similarity functions between objects.

The challenge aims to evaluate the *fitness* between:

- **Metric Definition** (text embedding)
- **Promptâ€“Response Pair** (text embedding)

The goal is to predict a **score between 0 and 10** that represents how well the promptâ€“response pair aligns with a specific evaluation metric.

### Why This Matters
In practical conversational AI evaluation:
- We need **high-quality test datasets**
- Each test prompt must match the **intended evaluation metric**
- Manual evaluation is expensive
- LLM-based automatic scoring introduces variability

This task allows us to build an automated scoring model that:
- Predicts similarity between two embeddings  
- Helps evaluate AI systems reliably  
- Improves dataset quality and testing coverage  

---

## Dataset

The dataset contains:

### Metric Name Embeddings (`metric_name_embeddings.npy`)
Vector representation (via Gemma Embedding Model) of each metric/submetric.

### Train Data (`train_data.json`)
Contains:
- **metric_name**
- **user_prompt**
- **response**
- **system_prompt**
- **score (0â€“10)**

### Test Embeddings  
Created by you using SentenceTransformer:
- `test_metric_embs.npy`
- `test_text_embs.npy`

These correspond to metric definitions and prompt-response pairs for test samples.

---

## Feature Engineering

For each train/test item, we compute:

1. **Cosine similarity**  
2. **Absolute difference** (embedding-wise)  
3. **Element-wise product**  
4. **Concatenation** of embeddings  
5. Final feature vector size â‰ˆ 3073  

This provides the model with both interaction features and raw representations.

---

## Model: ResNet-MLP

A custom deep MLP with:

- Input Projection  
- **8 Residual MLP Blocks**  
- Pre-LayerNorm  
- GELU Activation  
- Dropout  
- LayerNorm + Final Linear Head  

The architecture is highly stable for large dense vectors.

---

## Loss Function

We use a combination:

```
Total Loss = MSE + Î» * KL Histogram Loss
```

### ðŸ”¸ MSE Loss  
Ensures accuracy on individual samples.

### ðŸ”¸ KL Histogram Loss  
Ensures predicted score distribution matches target score PDF  
(prevents collapse, improves calibration).

---

##  Training Strategy

###  5-Fold Cross-Validation  
Prevents overfitting, provides robust OOF predictions.

###  SWA (Stochastic Weight Averaging)  
Smooths weights for better generalization.

###  EMA (Exponential Moving Average)  
Creates an additional stabilized model.

### Cosine Annealing LR  
Gradually reduces learning rate.

###  Early Stopping  
Triggers if validation RMSE stops improving.

---

## ðŸ“Š Post-Processing

### âœ” Linear Calibration  
Fits:
```
y = a * prediction + b
```

### âœ” Per-Fold Quantile Mapping  
For each fold:
- Align prediction distribution with target distribution  
- Stabilizes test predictions  

### âœ” Final Averaging + Clipping  
Mean of all folds  
Values clipped to `[0, 10]`.

---

## ðŸ“„ Final Submission

The generated file:

```
submission_resnetmlp_histKL_swa_ema_perfold.csv
```

Contains:

```
ID, score
1, 8.57
2, 6.44
...
```

---

## ðŸ“Š Visualizations

You produced two categories:

### **1. Dataset Visualization**
Saved in `figures_dataset/`:
- PCA & t-SNE of metric embeddings  
- Cosine similarity heatmap  
- Prompt/response length distributions  
- Score distributions  

### **2. Final Submission Visualization**
Saved in `figures_submission/`:
- Prediction histogram  
- KDE plot  
- Boxplot  
- Violin plot  
- CDF plot  
- Outlier detection  
- Score bucket heatmap  

---

## ðŸŒŸ Conclusion

This project successfully implements a **high-performance metric learning pipeline** using advanced deep learning techniques such as:

- ResNet-style MLP  
- KL divergence distribution matching  
- SWA & EMA  
- K-fold CV  
- Quantile mapping  
- Calibration  

It produces highly reliable fitness predictions for conversational AI evaluation tasks.

---

